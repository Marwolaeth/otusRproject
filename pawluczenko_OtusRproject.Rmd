---
title: "Оценка факторов размера заработной платы для отдельных профессий в Москве"
author: "Andrej Pawluczenko"
date: "08.01.2020"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 140)
options(scipen = 999999999)
```

## Введение

### Цели и задачи

Все представленное ниже подготовлено для курсового проекта по курсу «[Прикладная аналитика на R](https://otus.ru/lessons/r-for-analytics/)» на платформе Otus. Цель —проверить и закрепить полученные навыки на интересных данных.

В качестве задачи я выбрал оценку значимости факторов, влияющих на размер заработной платы, так как мне близка тема рынка труда и человеческих ресурсов. В качестве данных использована выгрузка из [API](https://github.com/hhru/api) портала HeadHunter. В выгрузке содержатся данные из объявлений о вакансиях для нескольких выбранных наугад профессий в Москве за декабрь 2019 — начало января 2020 гг.

### Что получилось

Несмотря на слабость итоговой модели и скудость оформления, считаю, что у меня получилось:
1. Получать (итеративно) данные из api по поисковому запросу и привести их в опрятный формат.
2. Выделить неочевидные признаки из «сырых» описаний вакансий, некоторые из которых оказались довольно информативными.
3. Построить конвейер двухуровневой оценки коэффициентов регрессии для каждой из отобранных профессий в отдельности.
4. Погдотовить, как мне кажется, достаточно удобный пайплайн, позволяющий в любой момент повторить процесс от начала до конца, за исключением особо затратных операций (парсинг вакансий или обработка текста).

**Не удалось**, к сожалению, в полной мере реализовать перспективный при таких данных, как наши, метод [fusion lasso](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!), реализованный в библиотеке [smurf](https://cran.r-project.org/web/packages/smurf/vignettes/smurf.html).

## Ход проекта

С кодом, подготовленным для проекта, можно ознакомиться в [репозитории](https://github.com/Marwolaeth/otusRproject) на GitHub. Здесь приведу только примеры, не требующие загрузки и обработки тяжелых данных.

### Поиск и подгрузка вакансий

Для удобства и репродуцируемости результатов написаны несколько функций для работы с API HH, которые:

* ищут идентификаторы вакансий по поисковому запросу;
* загружают и обрабатывают подходящие вакансии;
* загружают дополнительную информацию о работодателе.

Также в папке хранится файлы с векторами — идентификаторами уже загруженных вакансий и работодателей, чтобы не парсить их заново.

```{r parse_hh, warning=FALSE, message=FALSE}
require(purrr)
require(dplyr)
require(httr)
# Осторожно: следующий скрипт может установить много нежелательных библиотек!
source('jobs_01_tools.R', encoding = 'UTF-8')
source('jobs_02_functions.R', encoding = 'UTF-8')

# # идентификаторы вакансий, ранее уже загруженных в базу
# exist <- readRDS('data/exist.RDS')
# job_name = 'Дизайнер'
# 
# # К сожалению, при поиске вакансий за предыдущие дни,
# # Поиск может выдать ошибку
# # Советую указать более свежие даты или пропустить демонстрацию
# (q <- hh_set_query(
#   job_name,
#   date_from = '2020-01-27',
#   date_to = '2020-01-28'
# ))
# cat(sprintf('Ищем: «%s»...', job_name), '\n')
# vcs <- hh_vacancy_search(q, sleep = .1)
# cat(sprintf('   Найдено %d', length(vcs)), '\n')
# # Не загружаем уже загруженные вакансии
# vcs <- setdiff(vcs, exist)
# cat(sprintf('   Новых: %d. Парсинг...', length(vcs)), '\n')
# 
# if (length(vcs) > 0) {
#   vacancies <- vcs %>%
#     map(hh_get_vacancy) %>%
#     map(hh_parse_vacancy) %>%
#     bind_rows() %>%
#     mutate(job = job_name) %>%
#     select(id, job, everything())
#   
#     head(vacancies, 10)
# }
# 
# # идентификаторы работодателей для парсинга информации о них
# employer_ids   <- distinct(vacancies, employer.id) %>%
#   pull(employer.id)
# 
# if (length(employer_ids) > 0) {
#   employers <- employer_ids %>%
#     map(hh_get_employer) %>%
#     map(hh_parse_employer) %>%
#     bind_rows()
#   
#   head(employers, 10)
# }
```

Также была попытка загрузить больше информации о работодателе из внешних источников. Самым очевидным показался ресурс «[Викиданные](https://www.wikidata.org/)». Но несмотря на все затраты сил и времени идея оказалась неудачной: эти данные в силу своей скудости (или слабости функции поиска) совсем не пригодились в анализе. Продемонстрирую только несколько удачных примеров:

```{r parse_wikidata, warning=FALSE, message=FALSE}
# wikidata_parse_employer('Сбербанк')
# wikidata_parse_employer('Adecco Russia')
# wikidata_parse_employer("Озон", site = 'https://ozon.ru')
# wikidata_parse_employer('Императорский фарфоровый завод')
```

### Подготовка данных

На подготовке данных нет необходимости останавливаться подробно. Достаточно сказать, что использовался в основном диалект `tidyverse`.

### Разведывательный анализ

Разведывательный анализ данных с помощью библиотеки `dlookr` и визуализации показал, что среди имеющихся переменных потенциально хорошими предикторами являются:

* требуемый опыт работы;
* линия и/или станция метро (*например, станция «Деловой центр» показала высокий коэффициент, а линия МЦК — отрицательный*);
* тип работодателя, разместившего вакансию: кадровые агентства в среднем предлагали б́ольшую зарплату, чем прямые работодатели, руководители проектов и рекрутеры-фрилансеры.

Без внимания не осталась и разница в зарплатных предложениях между профессиями, что и демонстрируется на следующих диаграммах:

```{r plot_eda, warning=FALSE}
require(dlookr)
theme_set(theme_minimal() + theme(text = element_text(family = 'serif')))

df <- readRDS('data/headhunter_plus.RDS')
df <- df %>%
  mutate(
    salary = imputate_outlier(., salary, method = 'capping', no_attrs = TRUE)
  )
ggplot(df, aes(x = salary, fill = job)) +
  geom_density(alpha = .8, show.legend = FALSE) +
  facet_wrap(. ~ job) +
  scale_x_continuous(
    'Заработная плата (выбросы преобразованы в 95 процентиль)') +
  scale_y_continuous('Эмпирическая плотность распределения', labels = NULL) +
  ggtitle('Распределение размеров зарплат по профессиям')

ggplot(
  df,
  aes(x = job, y = salary, colour = experience, fill = experience)
) +
  geom_jitter(alpha = .05, width = .4, height = 1000) +
  # scale_y_log10() +
  stat_summary(
    fun.data = 'mean_cl_boot', geom = 'errorbar', lwd = .8, show.legend = TRUE
  ) +
  # stat_summary(fun.data = 'median_cl_boot', geom = 'errorbar', lwd = .8) +
  stat_summary(fun.y = 'mean', geom = 'point', pch = 23, size = 3) +
  # stat_summary(fun.y = 'median', geom = 'point', pch = 15, size = 3) +
  scale_x_discrete('Профессия (поисковый запрос к API HeadHunter)') +
  scale_y_continuous(
    'Заработная плата + 95% доверительный интервал\nдля среднего значения'
  ) +
  scale_colour_brewer(
    'Опыт работы:',
    palette = 'BuPu',
    aesthetics = c('colour', 'fill')
  ) +
  theme(axis.text.x = element_text(angle = 45)) +
  theme_dark() +
  ggtitle('Распределение зарплат по профессиям\nи требуемому опыту')
```

### Выделение признаков из текстовых переменных

Текстовые переменные в наших данных могли содержать:

* набор неопределенного количества представленных на сайте опций (ключевые навыки, специализации, отрасль компании-работодателя);
* неструктурированный текст (описания вакансий).

Эти четырех переменные поочередно мы:
* разбили на единицы (для опций — их отдельные значения, для описаний — слова и биграммы);
* выделили из них специфичные для каждой профессии.

Специфичность термина определялась вероятностью получить такое же или большее количество вхождений термина в данный класс (в нашем случае профессию) при гипергеометрическом рапределении. Функция представлена в библиотеке [R.temis](https://cran.r-project.org/web/packages/R.temis/index.html). Для каждого типа термина был свой порог значимости: самый легкий — для ключевых навыков, самый строгий — для ключевых слов из описаний. Топ специфичных терминов для каждой профессии представлен на диаграммах ниже:

```{r plot_specific,warning=FALSE}
dict_features <- readRDS('data/textual/feature_dictionary.RDS') %>%
  # «Наказание» для слишком больших шансов из-за гигансткого разрыва между ними
  # В целях опрятности графиков
  mutate(
    odds_job = if_else(
      odds_job >= quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE),
      (quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE) +
         log(odds_job) +
         runif(length(odds_job), 0, 6)
      ),
      odds_job
    ),
    fname = str_wrap(str_remove(fname, '^.+:\\s'), 40)
  ) %>%
  filter(!is.na(job)) %>%
  group_by(job, ftype) %>%
  top_n(15, odds_job) %>%
  split(.$job) %>%
  map(
    ~ split(., .$ftype) %>%
      set_names(c('Ключевые слова', 'Навыки', 'Специализации'))
  ) %>%
  map(~ map(., mutate, fname = forcats::fct_reorder(as.factor(fname), odds_job)))

category_colours <- data.frame(
  category = names(dict_features[[1]]),
  colour   = c('#33BBEE', '#CC3311', '#0077BB')
)
group_grid <- expand.grid(
  category = category_colours$category,
  g = names(dict_features)
) %>%
    left_join(category_colours)

# Осторожно: 21 диаграмма!!
# pmap(
#   group_grid,
#   plot_specific_features,
#   fvar = 'odds_job',
#   fvar.caption = 'Шанс упоминания в профессии',
#   l = dict_features
# )

pmap(
  group_grid[1:2,],
  plot_specific_features,
  fvar = 'odds_job',
  fvar.caption = 'Шанс упоминания в профессии',
  l = dict_features
)
```

Кроме того,рассчитано еще два параметра на основе текста описания вакансий: длина описания и тональность. Для расчета тональности использовался словарь из проекта [Kartaslov](https://github.com/dkulagin/kartaslov/) и тональный словарь английского языка из библиотеки `tidytext`.

Как видно, распределение тональности описаний очень похоже на нормальное, что объясняется Центральной предельной теоремой: значение тональности для каждого описания — это средняя тональность по словам.

```{r sentiment}
kartaslov_emo_dict <- read.csv(
  'tools/emo_dict.csv',
  sep = ';',
  dec = '.',
  na.strings = c('', ' '),
  colClasses = c('character', 'factor', rep('numeric', 6)),
  fileEncoding = 'UTF-8'
) %>%
  as_tibble()
kartaslov_emo_dict

tidytext::sentiments

ggplot(df, aes(x = description_sentiment)) +
  geom_density(alpha = .8, fill = 'blue') +
  scale_x_continuous('Тональность описания') +
  scale_y_continuous('Эмпирическая плотность распределения') +
  ggtitle('Распределение тональности описаний')
```

### Моделирование

Моделирование проходило в два этапа:

1. На первом с помощью lasso-регуляризации были отобраны релевантные для каждой из профессий текстовые признаки.
2. На втором регуляризация fusion lasso была применена ко всем переменным (исходные + извлеченные). Для каждой профессии строилась своя модель с использованием только релевантных для нее текстовых признаков.

Для отбора текстовых признаков использовалась функция `glmnet::cv.glmnet()` (собственная функция-обертка: `salary_glm_sparse()`). При финальном моделировании для реализации подход Fusion Lasso использовался пакет `smurf`, в котором реализованы методы, перечисленные, в частности, в этой [статье](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!). 

*Например, к переменной, содержащей название линии метро, на которой расположен рабочий офис, применялась регуляризация Generalised Fusion Lasso, позволяющий отбирать для регрессии только релевантные уровни категориальной переменной с множеством разных значений (как в случае с метро).*

Выделенный курсивом фрагмент не удалось реализовать из-за значительных ресурсозатрат, не оправданных соизмеримым ростом качества моделей.

## Результаты

Показатели качества получившихся моделей не впечатляют, что можно объяснить не только поверхностным знакомством автора с регуляризирующими алгоритмами, но и (для некоторых профессий) небольшим размером выборки. Примечательно, что наименее аккуратная модель рассчитана для профессии с наибольшим количеством наблюдений: менеджер по продажам.

### Показатели качества

```{r model_evaluation}
models_full <- readRDS('data/models/02f_variables.RDS')

models_full$model_full %>%
  map('accuracy') %>%
  set_names(models_full$job) %>%
  map(select, response, n, lambda, MAE, RMSE, R_sq.adj) %>%
map(
  set_names,
  c(
    'Y',
    'Кол-во наблюдений',
    'Лямбда',
    'MAE',
    'RMSE',
    'R_sq.adj'
  )
)
```

### Коэффициенты регрессии и их интерпретация

Получившиеся коэффициенты свидетельствуют, что извлеченные из текстовых переменных признаки внесли вклад в регрессию. К сожалению, некоторые значимые коэффициенты имеют явно нереалистичное и плохо интерпретируемое значение.

Рассмотрим коэффициенты для отдельных профессий и попробуем проинтерпретировать их. К сожалению, лассо-регрессия не дает информацию о значимости коэффициентов, так что мы можем судить о знаке и величине коэффициентов, но не о том, насколько точно модель оценивает его величину.

На диаграммах для каждой профессии из всех коэффициентов представлен топ по абсолютной величине.

```{r model_coefficients_01, fig.height=9, fig.width=9, warning=FALSE, dev='png'}
# Коэффициенты для Бухгалтера
plot_salary_coefficients('Бухгалтер')
```

Скорее всего, для позиции бухгалтера большое значение имеет опыт работы и навыки управления бухгалтерией и финансового менеджмента. Выделяется знание Международного стандарта финансовой отчетности (IFRS) — необходимый для международных транзакций и еще недавно относительно редкая специализация. Ключевое слово «менее 5» — скорее всего, артефакт, которых в матрицах данных немало, и который можно расшифровать как «не менее 5 лет».

Также среди положительных коэффициентов — специфическое ПО и предикат «отличное знание», вероятно, в разных описаниях имеющий разные значения аргумента. Среди коэффициентов с отрицательным значением — начало карьеры, термины кадрового делопроизводства и знание MS Excel (по контрасту с 1С).

```{r model_coefficients_02, fig.height=9, fig.width=9, warning=FALSE, dev='png'}
# Коэффициенты для Бухгалтера
plot_salary_coefficients('SMM-менеджер')
```

Для SMM-менеджеров высокий коэффициенты у ключевых слов SERM (модная специализация Search Engine Reputation Management) и YouTube, а отрицательные — у слова «мероприятие» и описаний с банальной формулировкой «умение писать».

Примечательно, что навыки SMM и Social Media Marketing имеют почти точно противоположные значения коэффициентов. Также бывает, что тот или иной навык имеет позитивный коэффициент, а полностью соответствующее ему ключевое слово — отрицательный. Можно предположить, что с размером заработной платы коррелирует также профессиональный уровень рекрутера, заполняющего объявления. Например, более профессиональный рекрутер использует более полные формулировки (‘Social Media Marketing’) и лучше использует раздел «навыки» (а не записывает все требования в блок текстового описания). Отчасти в наших моделях уже заложено такое предположение: переменная `description_length`.

## Заключение

В заключение изложу свои соображения, как можно улучшить модель:

* собрать больше наблюдений;
* использовать переменные `lat` и `lng` (соответственно широта и долгота места работы): например, создав на их основе квадраты или кластеры — регионы;
* более тонко настроить параметры регрессии.

Вывод: мне есть еще много чему учиться. Благодарю за внимание.
