<<<<<<< HEAD
---
title: "Оценка факторов размера заработной платы для отдельных профессий в Москве"
author: "Andrej Pawluczenko"
date: "08.01.2020"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Введение

### Цели и задачи

Все представленное ниже подготовлено для курсового проекта по курсу «[Прикладная аналитика на R](https://otus.ru/lessons/r-for-analytics/)» на платформе Otus. Цель —проверить и закрепить полученные навыки на интересных данных.

В качестве задачи я выбрал оценку значимости факторов, влияющих на размер заработной платы, так как мне близка тема рынка труда и человеческих ресурсов. В качестве данных использована выгрузка из [API](https://github.com/hhru/api) портала HeadHunter. В выгрузке содержатся данные из объявлений о вакансиях для нескольких выбранных наугад профессий в Москве за декабрь 2019 — начало января 2020 гг.

### Что получилось

Несмотря на слабость итоговой модели и скудость оформления, считаю, что у меня получилось:
1. Получать (итеративно) данные из api по поисковому запросу и привести их в опрятный формат.
2. Выделить неочевидные признаки из «сырых» описаний вакансий, некоторые из которых оказались довольно информативными.
3. Построить конвейер двухуровневой оценки коэффициентов регрессии для каждой из отобранных профессий в отдельности.
4. Погдотовить, как мне кажется, достаточно удобный пайплайн, позволяющий в любой момент повторить процесс от начала до конца, за исключением особо затратных операций (парсинг вакансий или обработка текста).

**Не удалось**, к сожалению, в полной мере реализовать перспективный при таких данных, как наши, метод [fusion lasso](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!), реализованный в библиотеке [smurf](https://cran.r-project.org/web/packages/smurf/vignettes/smurf.html).

## Ход проекта

С кодом, подготовленным для проекта, можно ознакомиться в [репозитории](https://github.com/Marwolaeth/otusRproject) на GitHub. Здесь приведу только примеры, не требующие загрузки и обработки тяжелых данных.

### Поиск и подгрузка вакансий

Для удобства и репродуцируемости результатов написаны несколько функций для работы с API HH, которые:

* ищут идентификаторы вакансий по поисковому запросу;
* загружают и обрабатывают подходящие вакансии;
* загружают дополнительную информацию о работодателе.

Также в папке хранится файлы с векторами — идентификаторами уже загруженных вакансий и работодателей, чтобы не парсить их заново.

```{r parse_hh, warning=FALSE, message=FALSE}
# Осторожно: следующий скрипт может установить много нежелательных библиотек!
source('jobs_01_tools.R')
source('jobs_02_functions.R')

exist <- readRDS('data/exist.RDS')
job_name = 'Дизайнер'

(q <- hh_set_query(
  job_name,
  date_from = '2020-01-09',
  date_to = '2020-01-10'
))
cat(sprintf('Ищем: «%s»...', job_name), '\n')
vcs <- hh_vacancy_search(q, sleep = .1)
cat(sprintf('   Найдено %d', length(vcs)), '\n')
# Не загружаем уже загруженные вакансии
vcs <- setdiff(vcs, exist)
cat(sprintf('   Новых: %d. Парсинг...', length(vcs)), '\n')

if (length(vcs) > 0) {
  vacancies <- vcs %>%
    map(hh_get_vacancy) %>%
    map(hh_parse_vacancy) %>%
    bind_rows() %>%
    mutate(job = job_name) %>%
    select(id, job, everything())
  
    head(vacancies, 10)
}

employer_ids   <- distinct(vacancies, employer.id) %>%
  pull(employer.id)

if (length(employer_ids) > 0) {
  employers <- employer_ids %>%
    map(hh_get_employer) %>%
    map(hh_parse_employer) %>%
    bind_rows()
  
  head(employers, 10)
}
```

Также была попытка загрузить больше информации о работодателе из внешних источников. Самым очевидным показался ресурс «[Викиданные](https://www.wikidata.org/)». Но несмотря на все затраты сил и времени идея оказалась неудачной: эти данные в силу своей скудости (или слабости функции поиска) совсем не пригодились в анализе. Продемонстрирую только несколько удачных примеров:

```{r parse_wikidata, warning=FALSE, message=FALSE}
wikidata_parse_employer('Сбербанк')
# На это раз NULL
wikidata_parse_employer('Procter&Gamble - Новомосковск')
wikidata_parse_employer('Adecco Russia')
wikidata_parse_employer("Озон", site = 'https://ozon.ru')
wikidata_parse_employer('Императорский фарфоровый завод')
```

### Подготовка данных

На подготовке данных нет необходимости останавливаться подробно. Достаточно сказать, что использовался в основном диалект `tidyverse`.

### Разведывательный анализ

Разведывательный анализ данных с помощью библиотеки `dlookr` и визуализации показал, что среди имеющихся переменных потенциально хорошими предикторами являются:

* требуемый опыт работы;
* линия и/или станция метро (*например, станция «Деловой центр» показала высокий коэффициент, а линия МЦК — отрицательный*);
* тип работодателя, разместившего вакансию: кадровые агентства в среднем предлагали б́ольшую зарплату, чем прямые работодатели, руководители проектов и рекрутеры-фрилансеры.

Без внимания не осталась и разница в зарплатных предложениях между профессиями, что и демонстрируется на следующих диаграммах:

```{r plot_eda, warning=FALSE}
require(dlookr)
options(scipen = 999999999)
theme_set(theme_minimal())

df <- readRDS('data/headhunter_plus.RDS')
df <- df %>%
  # Замена выбросов на 95 процентиль:
  mutate(
    salary = imputate_outlier(., salary, method = 'capping', no_attrs = TRUE)
  )
ggplot(df, aes(x = salary, fill = job)) +
  geom_density(alpha = .8, show.legend = FALSE) +
  facet_wrap(. ~ job) +
  scale_x_continuous(
    'Заработная плата (выбросы преобразованы в 95 процентиль)') +
  scale_y_continuous('Эмпирическая плотность распределения', labels = NULL) +
  ggtitle('Распределение размеров зарплат по профессиям')

ggplot(
  df,
  aes(x = job, y = salary, colour = experience, fill = experience)
) +
  geom_jitter(alpha = .1, width = .4, height = 1000) +
  # scale_y_log10() +
  stat_summary(
    fun.data = 'mean_cl_boot', geom = 'errorbar', lwd = .8, show.legend = TRUE
  ) +
  # stat_summary(fun.data = 'median_cl_boot', geom = 'errorbar', lwd = .8) +
  stat_summary(fun.y = 'mean', geom = 'point', pch = 23, size = 3) +
  # stat_summary(fun.y = 'median', geom = 'point', pch = 15, size = 3) +
  scale_x_discrete('Профессия (поисковый запрос к API HeadHunter)') +
  scale_y_continuous(
    'Заработная плата + 95% доверительный интервал для среднего значения'
  ) +
  theme(axis.text.x = element_text(angle = 45)) +
  ggtitle('Распределение зарплат по профессиям\nи требуемому опыту')
```

### Выделение признаков из текстовых переменных

Текстовые переменные в наших данных могли содержать:

* набор неопределенного количества представленных на сайте опций (ключевые навыки, специализации, отрасль компании-работодателя);
* неструктурированный текст (описания вакансий).

Эти четырех переменные поочередно мы:
* разбили на единицы (для опций — их отдельные значения, для описаний — слова и биграммы);
* выделили из них специфичные для каждой профессии.

Специфичность термина определялась вероятностью получить такое же или большее количество вхождений термина в данный класс (в нашем случае профессию) при гипергеометрическом рапределении. Функция представлена в библиотеке [R.temis](https://cran.r-project.org/web/packages/R.temis/index.html). Для каждого типа термина был свой порог значимости: самый легкий — для ключевых навыков, самый строгий — для ключевых слов из описаний. Топ специфичных терминов для каждой профессии представлен на диаграммах ниже:

```{r plot_specific,warning=FALSE}
dict_features <- readRDS('data/textual/feature_dictionary.RDS') %>%
  # «Наказание» для слишком больших шансов из-за гигансткого разрыва между ними
  # В целях опрятности графиков
  mutate(
    odds_job = if_else(
      odds_job >= quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE),
      (quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE) +
         log(odds_job) +
         runif(length(odds_job), 0, 6)
      ),
      odds_job
    ),
    fname = str_wrap(str_remove(fname, '^.+:\\s'), 40)
  ) %>%
  filter(!is.na(job)) %>%
  group_by(job, ftype) %>%
  top_n(15, odds_job) %>%
  split(.$job) %>%
  map(
    ~ split(., .$ftype) %>%
      set_names(c('Ключевые слова', 'Навыки', 'Специализации'))
  ) %>%
  map(~ map(., mutate, fname = forcats::fct_reorder(fname, odds_job)))

category_colours <- data.frame(
  category = names(dict_features[[1]]),
  colour   = c('#ea5e5e', '#6f9a8d', '#1f6650')
)
(group_grid <- expand.grid(
  category = category_colours$category,
  g = names(dict_features)
) %>%
    left_join(category_colours))

pmap(
  group_grid,
  plot_specific_features,
  fvar = 'odds_job',
  fvar.caption = 'Шанс упоминания в профессии',
  l = dict_features
)
```
=======
---
title: "Оценка факторов размера заработной платы для отдельных профессий в Москве"
author: "Andrej Pawluczenko"
date: "08.01.2020"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 140)
options(scipen = 999999999)
```

## Введение

### Цели и задачи

Все представленное ниже подготовлено для курсового проекта по курсу «[Прикладная аналитика на R](https://otus.ru/lessons/r-for-analytics/)» на платформе Otus. Цель —проверить и закрепить полученные навыки на интересных данных.

В качестве задачи я выбрал оценку значимости факторов, влияющих на размер заработной платы, так как мне близка тема рынка труда и человеческих ресурсов. В качестве данных использована выгрузка из [API](https://github.com/hhru/api) портала HeadHunter. В выгрузке содержатся данные из объявлений о вакансиях для нескольких выбранных наугад профессий в Москве за декабрь 2019 — начало января 2020 гг.

### Что получилось

Несмотря на слабость итоговой модели и скудость оформления, считаю, что у меня получилось:
1. Получать (итеративно) данные из api по поисковому запросу и привести их в опрятный формат.
2. Выделить неочевидные признаки из «сырых» описаний вакансий, некоторые из которых оказались довольно информативными.
3. Построить конвейер двухуровневой оценки коэффициентов регрессии для каждой из отобранных профессий в отдельности.
4. Погдотовить, как мне кажется, достаточно удобный пайплайн, позволяющий в любой момент повторить процесс от начала до конца, за исключением особо затратных операций (парсинг вакансий или обработка текста).

**Не удалось**, к сожалению, в полной мере реализовать перспективный при таких данных, как наши, метод [fusion lasso](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!), реализованный в библиотеке [smurf](https://cran.r-project.org/web/packages/smurf/vignettes/smurf.html).

## Ход проекта

С кодом, подготовленным для проекта, можно ознакомиться в [репозитории](https://github.com/Marwolaeth/otusRproject) на GitHub. Здесь приведу только примеры, не требующие загрузки и обработки тяжелых данных.

### Поиск и подгрузка вакансий

Для удобства и репродуцируемости результатов написаны несколько функций для работы с API HH, которые:

* ищут идентификаторы вакансий по поисковому запросу;
* загружают и обрабатывают подходящие вакансии;
* загружают дополнительную информацию о работодателе.

Также в папке хранится файлы с векторами — идентификаторами уже загруженных вакансий и работодателей, чтобы не парсить их заново.

```{r parse_hh, warning=FALSE, message=FALSE}
require(purrr)
require(dplyr)
require(httr)
# Осторожно: следующий скрипт может установить много нежелательных библиотек!
source('jobs_01_tools.R')
source('jobs_02_functions.R')

exist <- readRDS('data/exist.RDS')
job_name = 'Дизайнер'

# К сожалению, при поиске вакансий за предыдущие дни,
# Поиск может выдать ошибку
# Советую указать более свежие даты или пропустить демонстрацию
(q <- hh_set_query(
  job_name,
  date_from = '2020-01-16',
  date_to = '2020-01-17'
))
cat(sprintf('Ищем: «%s»...', job_name), '\n')
vcs <- hh_vacancy_search(q, sleep = .1)
cat(sprintf('   Найдено %d', length(vcs)), '\n')
# Не загружаем уже загруженные вакансии
vcs <- setdiff(vcs, exist)
cat(sprintf('   Новых: %d. Парсинг...', length(vcs)), '\n')

if (length(vcs) > 0) {
  vacancies <- vcs %>%
    setdiff(exist) %>%
    map(hh_get_vacancy) %>%
    map(hh_parse_vacancy) %>%
    bind_rows() %>%
    mutate(job = job_name) %>%
    select(id, job, everything())
  
    head(vacancies, 10)
}

employer_ids   <- distinct(vacancies, employer.id) %>%
  pull(employer.id)

if (length(employer_ids) > 0) {
  employers <- employer_ids %>%
    map(hh_get_employer) %>%
    map(hh_parse_employer) %>%
    bind_rows()
  
  head(employers, 10)
}
```

Также была попытка загрузить больше информации о работодателе из внешних источников. Самым очевидным показался ресурс «[Викиданные](https://www.wikidata.org/)». Но несмотря на все затраты сил и времени идея оказалась неудачной: эти данные в силу своей скудости (или слабости функции поиска) совсем не пригодились в анализе. Продемонстрирую только несколько удачных примеров:

```{r parse_wikidata, warning=FALSE, message=FALSE}
wikidata_parse_employer('Сбербанк')
wikidata_parse_employer('Adecco Russia')
wikidata_parse_employer("Озон", site = 'https://ozon.ru')
wikidata_parse_employer('Императорский фарфоровый завод')
```

### Подготовка данных

На подготовке данных нет необходимости останавливаться подробно. Достаточно сказать, что использовался в основном диалект `tidyverse`.

### Разведывательный анализ

Разведывательный анализ данных с помощью библиотеки `dlookr` и визуализации показал, что среди имеющихся переменных потенциально хорошими предикторами являются:

* требуемый опыт работы;
* линия и/или станция метро (*например, станция «Деловой центр» показала высокий коэффициент, а линия МЦК — отрицательный*);
* тип работодателя, разместившего вакансию: кадровые агентства в среднем предлагали б́ольшую зарплату, чем прямые работодатели, руководители проектов и рекрутеры-фрилансеры.

Без внимания не осталась и разница в зарплатных предложениях между профессиями, что и демонстрируется на следующих диаграммах:

```{r plot_eda, warning=FALSE}
require(dlookr)
theme_set(theme_minimal())

df <- readRDS('data/headhunter_plus.RDS')
df <- df %>%
  mutate(
    salary = imputate_outlier(., salary, method = 'capping', no_attrs = TRUE),
    description_sentiment = scale(description_sentiment)[,1]
  )
ggplot(df, aes(x = salary, fill = job)) +
  geom_density(alpha = .8, show.legend = FALSE) +
  facet_wrap(. ~ job) +
  scale_x_continuous(
    'Заработная плата (выбросы преобразованы в 95 процентиль)') +
  scale_y_continuous('Эмпирическая плотность распределения', labels = NULL) +
  ggtitle('Распределение размеров зарплат по профессиям')

ggplot(
  df,
  aes(x = job, y = salary, colour = experience, fill = experience)
) +
  geom_jitter(alpha = .1, width = .4, height = 1000) +
  # scale_y_log10() +
  stat_summary(
    fun.data = 'mean_cl_boot', geom = 'errorbar', lwd = .8, show.legend = TRUE
  ) +
  # stat_summary(fun.data = 'median_cl_boot', geom = 'errorbar', lwd = .8) +
  stat_summary(fun.y = 'mean', geom = 'point', pch = 23, size = 3) +
  # stat_summary(fun.y = 'median', geom = 'point', pch = 15, size = 3) +
  scale_x_discrete('Профессия (поисковый запрос к API HeadHunter)') +
  scale_y_continuous(
    'Заработная плата + 95% доверительный интервал для среднего значения'
  ) +
  scale_colour_ordinal('Опыт работы:', aesthetics = c('colour', 'fill')) +
  theme(axis.text.x = element_text(angle = 45)) +
  ggtitle('Распределение зарплат по профессиям\nи требуемому опыту')
```

### Выделение признаков из текстовых переменных

Текстовые переменные в наших данных могли содержать:

* набор неопределенного количества представленных на сайте опций (ключевые навыки, специализации, отрасль компании-работодателя);
* неструктурированный текст (описания вакансий).

Эти четырех переменные поочередно мы:
* разбили на единицы (для опций — их отдельные значения, для описаний — слова и биграммы);
* выделили из них специфичные для каждой профессии.

Специфичность термина определялась вероятностью получить такое же или большее количество вхождений термина в данный класс (в нашем случае профессию) при гипергеометрическом рапределении. Функция представлена в библиотеке [R.temis](https://cran.r-project.org/web/packages/R.temis/index.html). Для каждого типа термина был свой порог значимости: самый легкий — для ключевых навыков, самый строгий — для ключевых слов из описаний. Топ специфичных терминов для каждой профессии представлен на диаграммах ниже:

```{r plot_specific,warning=FALSE}
dict_features <- readRDS('data/textual/feature_dictionary.RDS') %>%
  # «Наказание» для слишком больших шансов из-за гигансткого разрыва между ними
  # В целях опрятности графиков
  mutate(
    odds_job = if_else(
      odds_job >= quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE),
      (quantile(
        odds_job, .75, na.rm = TRUE) + 1.5 * IQR(odds_job, na.rm = TRUE) +
         log(odds_job) +
         runif(length(odds_job), 0, 6)
      ),
      odds_job
    ),
    fname = str_wrap(str_remove(fname, '^.+:\\s'), 40)
  ) %>%
  filter(!is.na(job)) %>%
  group_by(job, ftype) %>%
  top_n(15, odds_job) %>%
  split(.$job) %>%
  map(
    ~ split(., .$ftype) %>%
      set_names(c('Ключевые слова', 'Навыки', 'Специализации'))
  ) %>%
  map(~ map(., mutate, fname = forcats::fct_reorder(as.factor(fname), odds_job)))

category_colours <- data.frame(
  category = names(dict_features[[1]]),
  colour   = c('#ea5e5e', '#6f9a8d', '#1f6650')
)
group_grid <- expand.grid(
  category = category_colours$category,
  g = names(dict_features)
) %>%
    left_join(category_colours)

# Осторожно: 21 диаграмма!!
# pmap(
#   group_grid,
#   plot_specific_features,
#   fvar = 'odds_job',
#   fvar.caption = 'Шанс упоминания в профессии',
#   l = dict_features
# )

pmap(
  group_grid[1:2,],
  plot_specific_features,
  fvar = 'odds_job',
  fvar.caption = 'Шанс упоминания в профессии',
  l = dict_features
)
```

Кроме того,рассчитано еще два параметра на основе текста описания вакансий: длина описания и тональность. Для расчета тональности использовался словарь из проекта [Kartaslov](https://github.com/dkulagin/kartaslov/) и тональный словарь английского языка из библиотеки `tidytext`.

```{r sentiment}
kartaslov_emo_dict <- read.csv(
  'tools/emo_dict.csv',
  sep = ';',
  dec = '.',
  na.strings = c('', ' '),
  colClasses = c('character', 'factor', rep('numeric', 6)),
  fileEncoding = 'UTF-8'
) %>%
  as_tibble()
kartaslov_emo_dict

tidytext::sentiments

ggplot(df, aes(x = description_sentiment)) +
  geom_density(alpha = .8, fill = 'blue') +
  scale_x_continuous('Тональность описания') +
  scale_y_continuous('Эмпирическая плотность распределения') +
  ggtitle('Распределение тональности описаний')
```

### Моделирование

Моделирование проходило в два этапа:

1. На первом с помощью lasso-регуляризации были отобраны релевантные для каждой из профессий текстовые признаки.
2. На втором регуляризация fused lasso была применена ко всем переменным (исходные + извлеченные). Для каждой профессии строилась своя модель с использованием только релевантных для нее текстовых признаков.

Для отбора текстовых признаков использовалась функция `glmnet::cv.glmnet()` (собственная функция-обертка: `salary_glm_sparse()`). При финальном моделировании для реализации подход Fused Lasso использовался пакет `smurf`, в котором реализованы методы, перечисленные, в частности, в этой [статье](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!). Например, к переменной, содержащей название линии метро, на которой расположен рабочий офис, применялась регуляризация Generalised Fused Lasso, позволяющий отбирать для регрессии только релевантные уровни категориальной переменной с множеством разных значений (как в случае с метро).

## Результаты

Показатели качества получившихся моделей не впечатляют, что можно объяснить не только поверхностным знакомством автора с регуляризирующими алгоритмами, но и (для некоторых профессий) небольшим размером выборки.

### Показатели качества

```{r model_evaluation}
models_full <- readRDS('data/models/02b_variables.RDS')

models_full$model_full %>%
  map('accuracy') %>%
  set_names(models_full$job) %>%
  map(select, response, n, lambda, mean_abs_error, RMSE, R_sq.adj) %>%
map(
  set_names,
  c(
    'Зависимая переменная',
    'Количество наблюдений',
    'Вычисленная лямбда',
    'Средняя абсолютная ошибка',
    'Среднеквадратичная ошибка',
    'R_sq.adj'
  )
)
```

### Коэффициенты регрессии

Получившиеся коэффициенты свидетельствуют, что извлеченные из текстовых переменных признаки внесли вклад в регрессию. К сожалению, некоторые значимые коэффициенты имеют явно нереалистичное и плохо интерпретируемое значение.

Очень сожалею, что не успел изобразить коэффициенты графически.

```{r model_coefficients, warning=FALSE}
coefs <- models_full$model_full %>%
  map('coefficients') %>%
  set_names(models_full$job) %>%
  map(select, fname, ftype, beta) %>%
  map(~ mutate(., fname = str_remove(fname, '^.+:\\s'))) %>%
  map(filter, abs(beta) > 1000)

# Осторожно: длинная выдача
# for (i in seq_along(coefs)) {
#   cat(paste0(models_full$job[i], ':'))
#   print(coefs[[i]], n = 50, width = 100)
# }

# Коэффициенты для Бухгалтера
print(coefs[[2]], n = 50, width = 100)

```

## Заключение

В заключение изложу свои соображения, как можно улучшить модель:

* собрать больше наблюдений;
* использовать переменные `lat` и `lng` (соответственно широта и долгота места работы): например, создав на их основе квадраты-регионы;
* более тонко настроить параметры регрессии.

Вывод: мне есть еще много чему учиться. Благодарю за внимание.
>>>>>>> fe5cacbbbf91b1954b6382c3b15b2145bca92d49
