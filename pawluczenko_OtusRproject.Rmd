---
title: "Оценка факторов размера заработной платы для отдельных профессий в Москве"
author: "Andrej Pawluczenko"
date: "08.01.2020"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Введение

### Цели и задачи

Все представленное ниже подготовлено для курсового проекта по курсу «[Прикладная аналитика на R](https://otus.ru/lessons/r-for-analytics/)» на платформе Otus. Цель —проверить и закрепить полученные навыки на интересных данных.

В качестве задачи я выбрал оценку значимости факторов, влияющих на размер заработной платы, так как мне близка тема рынка труда и человеческих ресурсов. В качестве данных использована выгрузка из [API](https://github.com/hhru/api) портала HeadHunter. В выгрузке содержатся данные из объявлений о вакансиях для нескольких выбранных наугад профессий в Москве за декабрь 2019 — начало января 2020 гг.

### Что получилось

Несмотря на слабость итоговой модели и скудость оформления, считаю, что у меня получилось:
1. Получать (итеративно) данные из api по поисковому запросу и привести их в опрятный формат.
2. Выделить неочевидные признаки из «сырых» описаний вакансий, некоторые из которых оказались довольно информативными.
3. Построить конвейер двухуровневой оценки коэффициентов регрессии для каждой из отобранных профессий в отдельности.
4. Погдотовить, как мне кажется, достаточно удобный пайплайн, позволяющий в любой момент повторить процесс от начала до конца, за исключением особо затратных операций (парсинг вакансий или обработка текста).

**Не удалось**, к сожалению, в полной мере реализовать перспективный при таких данных, как наши, метод [fusion lasso](https://pdfs.semanticscholar.org/f73f/ca3a80099dc649c81d9a59c995c315eaab6a.pdf)(pdf!), реализованный в библиотеке [smurf](https://cran.r-project.org/web/packages/smurf/vignettes/smurf.html).

## Ход проекта

С кодом, подготовленным для проекта, можно ознакомиться в [репозитории](https://github.com/Marwolaeth/otusRproject) на GitHub. Здесь приведу только примеры, не требующие загрузки и обработки тяжелых данных.

### Поиск и подгрузка вакансий

Для удобства и репродуцируемости результатов написаны несколько функций для работы с API HH, которые:

* ищут идентификаторы вакансий по поисковому запросу;
* загружают и обрабатывают подходящие вакансии;
* загружают дополнительную информацию о работодателе.

Также в папке хранится файлы с векторами — идентификаторами уже загруженных вакансий и работодателей, чтобы не парсить их заново.

```{r parse_hh, warning=FALSE, message=FALSE}
require(purrr)
require(dplyr)
require(httr)
source('jobs_02_functions.R')

exist <- readRDS('data/exist.RDS')
job_name = 'Дизайнер'

(q <- hh_set_query(
  job_name,
  date_from = '2020-01-07',
  date_to = '2020-01-08'
))
cat(sprintf('Ищем: «%s»...', job_name), '\n')
vcs <- hh_vacancy_search(q, sleep = .1)
cat(sprintf('   Найдено %d', length(vcs)), '\n')
# Не загружаем уже загруженные вакансии
vcs <- setdiff(vcs, exist)
cat(sprintf('   Новых: %d. Парсинг...', length(vcs)), '\n')

if (length(vcs) > 0) {
  vacancies <- vcs %>%
    setdiff(exist) %>%
    map(hh_get_vacancy) %>%
    map(hh_parse_vacancy) %>%
    bind_rows() %>%
    mutate(job = job_name) %>%
    select(id, job, everything())
  
    head(vacancies, 10)
}

employer_ids   <- distinct(vacancies, employer.id) %>%
  pull(employer.id)

if (length(employer_ids) > 0) {
  employers <- employer_ids %>%
    map(hh_get_employer) %>%
    map(hh_parse_employer) %>%
    bind_rows()
  
  head(employers, 10)
}
```

Также была попытка загрузить больше информации о работодателе из внешних источников. Самым очевидным показался ресурс «[Викиданные](https://www.wikidata.org/)». Но несмотря на все затраты сил и времени идея оказалась неудачной: эти данные в силу своей скудости (или слабости функции поиска) совсем не пригодились в анализе. Продемонстрирую только несколько удачных примеров:

```{r parse_wikidata, warning=FALSE, message=FALSE}
wikidata_parse_employer('Сбербанк')
wikidata_parse_employer('Procter&Gamble - Новомосковск')
wikidata_parse_employer('Adecco Russia')
wikidata_parse_employer("Озон", site = 'https://ozon.ru')
wikidata_parse_employer('Императорский фарфоровый завод')
```